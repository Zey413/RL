{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gym\nenv = gym.make('CartPole-v0')","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!apt-get install --upgrade python-opengl -y","execution_count":22,"outputs":[{"output_type":"stream","text":"Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\npython-opengl is already the newest version (3.1.0+dfsg-1).\n0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !apt-get install --upgrade python-opengl -y\n!git clone https://github.com/openai/gym.git\n!cd gym\n!pip install -e .","execution_count":28,"outputs":[{"output_type":"stream","text":"fatal: destination path 'gym' already exists and is not an empty directory.\n\u001b[31mERROR: File \"setup.py\" not found. Directory cannot be installed in editable mode: /kaggle/working\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\n\n# install required system dependencies\napt-get install -y xvfb x11-utils\n\n# install required python dependencies (might need to install additional gym extras depending)\npip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*","execution_count":33,"outputs":[{"output_type":"stream","text":"Reading package lists...\nBuilding dependency tree...\nReading state information...\nx11-utils is already the newest version (7.7+3build1).\nx11-utils set to manually installed.\nxvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\nCollecting gym[box2d]==0.17.*\n  Downloading gym-0.17.3.tar.gz (1.6 MB)\nCollecting pyvirtualdisplay==0.2.*\n  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\nCollecting PyOpenGL==3.1.*\n  Downloading PyOpenGL-3.1.5-py3-none-any.whl (2.4 MB)\nCollecting PyOpenGL-accelerate==3.1.*\n  Downloading PyOpenGL-accelerate-3.1.5.tar.gz (538 kB)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym[box2d]==0.17.*) (1.5.4)\nRequirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym[box2d]==0.17.*) (1.19.5)\nRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym[box2d]==0.17.*) (1.5.0)\nRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[box2d]==0.17.*) (1.6.0)\nCollecting box2d-py~=2.3.5\n  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\nCollecting EasyProcess\n  Downloading EasyProcess-0.3-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.18.2)\nBuilding wheels for collected packages: gym, PyOpenGL-accelerate\n  Building wheel for gym (setup.py): started\n  Building wheel for gym (setup.py): finished with status 'done'\n  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654653 sha256=79b6ddf3403f27a23fa1d1d8cf1e4735a8206ab4dabb1468321d231a6faf30b3\n  Stored in directory: /root/.cache/pip/wheels/d1/81/4b/dd9c029691022cb957398d1f015e66b75e37637dda61abdf58\n  Building wheel for PyOpenGL-accelerate (setup.py): started\n  Building wheel for PyOpenGL-accelerate (setup.py): finished with status 'done'\n  Created wheel for PyOpenGL-accelerate: filename=PyOpenGL_accelerate-3.1.5-cp37-cp37m-linux_x86_64.whl size=1602320 sha256=1f116f4351f535515f7dfb4fed67c80709e1df4ea8321e7c752e2ac2dc7bb651\n  Stored in directory: /root/.cache/pip/wheels/1c/f5/6f/169afb3f2d476c5e807f8515b3c9bc9b819c3962316aa804eb\nSuccessfully built gym PyOpenGL-accelerate\nInstalling collected packages: gym, EasyProcess, box2d-py, pyvirtualdisplay, PyOpenGL-accelerate, PyOpenGL\n  Attempting uninstall: gym\n    Found existing installation: gym 0.18.0\n    Uninstalling gym-0.18.0:\n      Successfully uninstalled gym-0.18.0\nSuccessfully installed EasyProcess-0.3 PyOpenGL-3.1.5 PyOpenGL-accelerate-3.1.5 box2d-py-2.3.8 gym-0.17.3 pyvirtualdisplay-0.2.5\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyvirtualdisplay\n\n\n_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n                                    size=(1400, 900))\n_ = _display.start()","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple gym[atari]","execution_count":38,"outputs":[{"output_type":"stream","text":"Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nRequirement already satisfied: gym[atari] in /opt/conda/lib/python3.7/site-packages (0.17.3)\nRequirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.19.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.5.4)\nRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.6.0)\nRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (1.5.0)\nCollecting atari-py~=0.2.0\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8f/ba/1d22e9d2f332f07aaa57041f5dd569c2cb40a92bd6374a0b743ec3dfae97/atari_py-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (2.8 MB)\n\u001b[K     |████████████████████████████████| 2.8 MB 1.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (4.5.1.48)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from gym[atari]) (7.2.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.18.2)\nInstalling collected packages: atari-py\nSuccessfully installed atari-py-0.2.6\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gym\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nenv = gym.make('CartPole-v0') # insert your favorite environment\nrender = lambda : plt.imshow(env.render(mode='rgb_array'))\nenv.reset()\nrender()\n# arr = env.render(mode='rgb_array')\n# plt.imshow(arr) or scipy.misc.imsave('sample.png', arr)","execution_count":40,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'base' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-62371ea1951e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# arr = env.render(mode='rgb_array')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# plt.imshow(arr) or scipy.misc.imsave('sample.png', arr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-62371ea1951e>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# insert your favorite environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'darwin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[0;31m# noqa: F821\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gym\nfrom IPython import display\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nenv = gym.make('Pong-v0')\nenv.reset()\nimg = plt.imshow(env.render(mode='rgb_array')) # only call this once\nfor _ in range(1000):\n    img.set_data(env.render(mode='rgb_array')) # just update the data\n    display.display(plt.gcf())\n    display.clear_output(wait=True)\n    action = env.action_space.sample()\n    env.step(action)","execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install nes_py","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting nes_py\n  Downloading nes_py-8.1.6.tar.gz (77 kB)\n\u001b[K     |████████████████████████████████| 77 kB 925 kB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /opt/conda/lib/python3.7/site-packages (from nes_py) (0.18.0)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.7/site-packages (from nes_py) (1.19.5)\nRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from nes_py) (1.5.0)\nRequirement already satisfied: tqdm>=4.48.2 in /opt/conda/lib/python3.7/site-packages (from nes_py) (4.55.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym>=0.17.2->nes_py) (1.5.4)\nRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym>=0.17.2->nes_py) (1.6.0)\nRequirement already satisfied: Pillow<=7.2.0 in /opt/conda/lib/python3.7/site-packages (from gym>=0.17.2->nes_py) (7.2.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->nes_py) (0.18.2)\nBuilding wheels for collected packages: nes-py\n  Building wheel for nes-py (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nes-py: filename=nes_py-8.1.6-cp37-cp37m-linux_x86_64.whl size=434003 sha256=6eb0e0dddff77b21d881aaaeb1ccd3b7536ddea562fc7853e2f4b607d5b0fd7a\n  Stored in directory: /root/.cache/pip/wheels/be/25/61/604ca10fbac432982868b6d3cbecad01687eb8019663ca869d\nSuccessfully built nes-py\nInstalling collected packages: nes-py\nSuccessfully installed nes-py-8.1.6\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install gym_super_mario_bros","execution_count":4,"outputs":[{"output_type":"stream","text":"Collecting gym_super_mario_bros\n  Downloading gym_super_mario_bros-7.3.2-py2.py3-none-any.whl (198 kB)\n\u001b[K     |████████████████████████████████| 198 kB 1.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: nes-py>=8.1.2 in /opt/conda/lib/python3.7/site-packages (from gym_super_mario_bros) (8.1.6)\nRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from nes-py>=8.1.2->gym_super_mario_bros) (1.5.0)\nRequirement already satisfied: tqdm>=4.48.2 in /opt/conda/lib/python3.7/site-packages (from nes-py>=8.1.2->gym_super_mario_bros) (4.55.1)\nRequirement already satisfied: gym>=0.17.2 in /opt/conda/lib/python3.7/site-packages (from nes-py>=8.1.2->gym_super_mario_bros) (0.18.0)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.7/site-packages (from nes-py>=8.1.2->gym_super_mario_bros) (1.19.5)\nRequirement already satisfied: Pillow<=7.2.0 in /opt/conda/lib/python3.7/site-packages (from gym>=0.17.2->nes-py>=8.1.2->gym_super_mario_bros) (7.2.0)\nRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym>=0.17.2->nes-py>=8.1.2->gym_super_mario_bros) (1.6.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym>=0.17.2->nes-py>=8.1.2->gym_super_mario_bros) (1.5.4)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->nes-py>=8.1.2->gym_super_mario_bros) (0.18.2)\nInstalling collected packages: gym-super-mario-bros\nSuccessfully installed gym-super-mario-bros-7.3.2\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport random\nfrom nes_py.wrappers import JoypadSpace\nimport gym_super_mario_bros\nfrom tqdm import tqdm\nimport pickle \nfrom gym_super_mario_bros.actions import RIGHT_ONLY\nimport gym\nimport numpy as np\nimport collections \nimport cv2\nimport matplotlib.pyplot as plt","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env=None, skip=4):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        super(MaxAndSkipEnv, self).__init__(env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = collections.deque(maxlen=2)\n        self._skip = skip\n\n    def step(self, action):\n        total_reward = 0.0\n        done = None\n        for _ in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            self._obs_buffer.append(obs)\n            total_reward += reward\n            if done:\n                break\n        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n        return max_frame, total_reward, done, info\n\n    def reset(self):\n        \"\"\"Clear past frame buffer and init to first obs\"\"\"\n        self._obs_buffer.clear()\n        obs = self.env.reset()\n        self._obs_buffer.append(obs)\n        return obs\n\n\nclass ProcessFrame84(gym.ObservationWrapper):\n    \"\"\"\n    Downsamples image to 84x84\n    Greyscales image\n\n    Returns numpy array\n    \"\"\"\n    def __init__(self, env=None):\n        super(ProcessFrame84, self).__init__(env)\n        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n\n    def observation(self, obs):\n        return ProcessFrame84.process(obs)\n\n    @staticmethod\n    def process(frame):\n        if frame.size == 240 * 256 * 3:\n            img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n        else:\n            assert False, \"Unknown resolution.\"\n        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n        x_t = resized_screen[18:102, :]\n        x_t = np.reshape(x_t, [84, 84, 1])\n        return x_t.astype(np.uint8)\n\n\nclass ImageToPyTorch(gym.ObservationWrapper):\n    def __init__(self, env):\n        super(ImageToPyTorch, self).__init__(env)\n        old_shape = self.observation_space.shape\n        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n                                                dtype=np.float32)\n\n    def observation(self, observation):\n        return np.moveaxis(observation, 2, 0)\n\n\nclass ScaledFloatFrame(gym.ObservationWrapper):\n    \"\"\"Normalize pixel values in frame --> 0 to 1\"\"\"\n    def observation(self, obs):\n        return np.array(obs).astype(np.float32) / 255.0\n\n\nclass BufferWrapper(gym.ObservationWrapper):\n    def __init__(self, env, n_steps, dtype=np.float32):\n        super(BufferWrapper, self).__init__(env)\n        self.dtype = dtype\n        old_space = env.observation_space\n        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n\n    def reset(self):\n        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n        return self.observation(self.env.reset())\n\n    def observation(self, observation):\n        self.buffer[:-1] = self.buffer[1:]\n        self.buffer[-1] = observation\n        return self.buffer\n\n\ndef make_env(env):\n    env = MaxAndSkipEnv(env)\n    env = ProcessFrame84(env)\n    env = ImageToPyTorch(env)\n    env = BufferWrapper(env, 4)\n    env = ScaledFloatFrame(env)\n    return JoypadSpace(env, RIGHT_ONLY)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DQNSolver(nn.Module):\n\n    def __init__(self, input_shape, n_actions):\n        super(DQNSolver, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.ReLU()\n        )\n\n        conv_out_size = self._get_conv_out(input_shape)\n        self.fc = nn.Sequential(\n            nn.Linear(conv_out_size, 512),\n            nn.ReLU(),\n            nn.Linear(512, n_actions)\n        )\n    \n    def _get_conv_out(self, shape):\n        o = self.conv(torch.zeros(1, *shape))\n        return int(np.prod(o.size()))\n\n    def forward(self, x):\n        conv_out = self.conv(x).view(x.size()[0], -1)\n        return self.fc(conv_out)\n    \n\nclass DQNAgent:\n\n    def __init__(self, state_space, action_space, max_memory_size, batch_size, gamma, lr,\n                 dropout, exploration_max, exploration_min, exploration_decay, double_dq, pretrained):\n\n        # Define DQN Layers\n        self.state_space = state_space\n        self.action_space = action_space\n        self.double_dq = double_dq\n        self.pretrained = pretrained\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        if self.double_dq:  \n            self.local_net = DQNSolver(state_space, action_space).to(self.device)\n            self.target_net = DQNSolver(state_space, action_space).to(self.device)\n            \n            if self.pretrained:\n                self.local_net.load_state_dict(torch.load(\"dq1.pt\", map_location=torch.device(self.device)))\n                self.target_net.load_state_dict(torch.load(\"dq2.pt\", map_location=torch.device(self.device)))\n                    \n            self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=lr)\n            self.copy = 5000  # Copy the local model weights into the target network every 5000 steps\n            self.step = 0\n        else:  \n            self.dqn = DQNSolver(state_space, action_space).to(self.device)\n            \n            if self.pretrained:\n                self.dqn.load_state_dict(torch.load(\"dq.pt\", map_location=torch.device(self.device)))\n            self.optimizer = torch.optim.Adam(self.dqn.parameters(), lr=lr)\n\n        # Create memory\n        self.max_memory_size = max_memory_size\n        if self.pretrained:\n            self.STATE_MEM = torch.load(\"STATE_MEM.pt\")\n            self.ACTION_MEM = torch.load(\"ACTION_MEM.pt\")\n            self.REWARD_MEM = torch.load(\"REWARD_MEM.pt\")\n            self.STATE2_MEM = torch.load(\"STATE2_MEM.pt\")\n            self.DONE_MEM = torch.load(\"DONE_MEM.pt\")\n            with open(\"ending_position.pkl\", 'rb') as f:\n                self.ending_position = pickle.load(f)\n            with open(\"num_in_queue.pkl\", 'rb') as f:\n                self.num_in_queue = pickle.load(f)\n        else:\n            self.STATE_MEM = torch.zeros(max_memory_size, *self.state_space)\n            self.ACTION_MEM = torch.zeros(max_memory_size, 1)\n            self.REWARD_MEM = torch.zeros(max_memory_size, 1)\n            self.STATE2_MEM = torch.zeros(max_memory_size, *self.state_space)\n            self.DONE_MEM = torch.zeros(max_memory_size, 1)\n            self.ending_position = 0\n            self.num_in_queue = 0\n        \n        self.memory_sample_size = batch_size\n        \n        # Learning parameters\n        self.gamma = gamma\n        self.l1 = nn.SmoothL1Loss().to(self.device) # Also known as Huber loss\n        self.exploration_max = exploration_max\n        self.exploration_rate = exploration_max\n        self.exploration_min = exploration_min\n        self.exploration_decay = exploration_decay\n\n    def remember(self, state, action, reward, state2, done):\n        self.STATE_MEM[self.ending_position] = state.float()\n        self.ACTION_MEM[self.ending_position] = action.float()\n        self.REWARD_MEM[self.ending_position] = reward.float()\n        self.STATE2_MEM[self.ending_position] = state2.float()\n        self.DONE_MEM[self.ending_position] = done.float()\n        self.ending_position = (self.ending_position + 1) % self.max_memory_size  # FIFO tensor\n        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n        \n    def recall(self):\n        # Randomly sample 'batch size' experiences\n        idx = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n        \n        STATE = self.STATE_MEM[idx]\n        ACTION = self.ACTION_MEM[idx]\n        REWARD = self.REWARD_MEM[idx]\n        STATE2 = self.STATE2_MEM[idx]\n        DONE = self.DONE_MEM[idx]\n        \n        return STATE, ACTION, REWARD, STATE2, DONE\n\n    def act(self, state):\n        # Epsilon-greedy action\n        \n        if self.double_dq:\n            self.step += 1\n        if random.random() < self.exploration_rate:  \n            return torch.tensor([[random.randrange(self.action_space)]])\n        if self.double_dq:\n            # Local net is used for the policy\n            return torch.argmax(self.local_net(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n        else:\n            return torch.argmax(self.dqn(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n\n    def copy_model(self):\n        # Copy local net weights into target net\n        \n        self.target_net.load_state_dict(self.local_net.state_dict())\n    \n    def experience_replay(self):\n        \n        if self.double_dq and self.step % self.copy == 0:\n            self.copy_model()\n\n        if self.memory_sample_size > self.num_in_queue:\n            return\n\n        STATE, ACTION, REWARD, STATE2, DONE = self.recall()\n        STATE = STATE.to(self.device)\n        ACTION = ACTION.to(self.device)\n        REWARD = REWARD.to(self.device)\n        STATE2 = STATE2.to(self.device)\n        DONE = DONE.to(self.device)\n        \n        self.optimizer.zero_grad()\n        if self.double_dq:\n            # Double Q-Learning target is Q*(S, A) <- r + γ max_a Q_target(S', a)\n            target = REWARD + torch.mul((self.gamma * \n                                        self.target_net(STATE2).max(1).values.unsqueeze(1)), \n                                        1 - DONE)\n\n            current = self.local_net(STATE).gather(1, ACTION.long()) # Local net approximation of Q-value\n        else:\n            # Q-Learning target is Q*(S, A) <- r + γ max_a Q(S', a) \n            target = REWARD + torch.mul((self.gamma * \n                                        self.dqn(STATE2).max(1).values.unsqueeze(1)), \n                                        1 - DONE)\n                \n            current = self.dqn(STATE).gather(1, ACTION.long())\n        \n        loss = self.l1(current, target)\n        loss.backward() # Compute gradients\n        self.optimizer.step() # Backpropagate error\n\n        self.exploration_rate *= self.exploration_decay\n        \n        # Makes sure that exploration rate is always at least 'exploration min'\n        self.exploration_rate = max(self.exploration_rate, self.exploration_min)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def vectorize_action(action, action_space):\n    # Given a scalar action, return a one-hot encoded action\n    \n    return [0 for _ in range(action)] + [1] + [0 for _ in range(action + 1, action_space)]","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_state(env, ep=0, info=\"\"):\n    plt.figure(3)\n    plt.clf()\n    plt.imshow(env.render(mode='rgb_array'))\n    plt.title(\"Episode: %d %s\" % (ep, info))\n    plt.axis('off')\n\n    display.clear_output(wait=True)\n    display.display(plt.gcf())","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(training_mode, pretrained):\n   \n    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n    env = make_env(env)  # Wraps the environment so that frames are grayscale \n    observation_space = env.observation_space.shape\n    action_space = env.action_space.n\n    agent = DQNAgent(state_space=observation_space,\n                     action_space=action_space,\n                     max_memory_size=30000,\n                     batch_size=32,\n                     gamma=0.90,\n                     lr=0.00025,\n                     dropout=0.,\n                     exploration_max=1.0,\n                     exploration_min=0.02,\n                     exploration_decay=0.99,\n                     double_dq=True,\n                     pretrained=pretrained)\n    \n    num_episodes = 10000\n    env.reset()\n    total_rewards = []\n    \n    for ep_num in tqdm(range(num_episodes)):\n        state = env.reset()\n        state = torch.Tensor([state])\n        total_reward = 0\n        steps = 0\n        while True:\n            if not training_mode:\n                show_state(env, ep_num)\n            action = agent.act(state)\n            steps += 1\n            \n            state_next, reward, terminal, info = env.step(int(action[0]))\n            total_reward += reward\n            state_next = torch.Tensor([state_next])\n            reward = torch.tensor([reward]).unsqueeze(0)\n            \n            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n            \n            if training_mode:\n                agent.remember(state, action, reward, state_next, terminal)\n                agent.experience_replay()\n            \n            state = state_next\n            if terminal:\n                break\n        \n        total_rewards.append(total_reward)\n\n        print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n        num_episodes += 1      \n    \n    if training_mode:\n        with open(\"ending_position.pkl\", \"wb\") as f:\n            pickle.dump(agent.ending_position, f)\n        with open(\"num_in_queue.pkl\", \"wb\") as f:\n            pickle.dump(agent.num_in_queue, f)\n        with open(\"total_rewards.pkl\", \"wb\") as f:\n            pickle.dump(total_rewards, f)\n        if agent.double_dq:\n            torch.save(agent.local_net.state_dict(), \"dq1.pt\")\n            torch.save(agent.target_net.state_dict(), \"dq2.pt\")\n        else:\n            torch.save(agent.dqn.state_dict(), \"dq.pt\")  \n        torch.save(agent.STATE_MEM,  \"STATE_MEM.pt\")\n        torch.save(agent.ACTION_MEM, \"ACTION_MEM.pt\")\n        torch.save(agent.REWARD_MEM, \"REWARD_MEM.pt\")\n        torch.save(agent.STATE2_MEM, \"STATE2_MEM.pt\")\n        torch.save(agent.DONE_MEM,   \"DONE_MEM.pt\")\n    \n    env.close()\n    \n    if num_episodes > 500:\n        plt.title(\"Episodes trained vs. Average Rewards (per 500 eps)\")\n        plt.plot([0 for _ in range(500)] + \n                 np.convolve(total_rewards, np.ones((500,))/500, mode=\"valid\").tolist())\n        plt.show()","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(training_mode=True, pretrained=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}